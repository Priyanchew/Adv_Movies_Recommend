{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3qNPoQUqHLb5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Load the MovieLens ratings data (assumed to have columns: userId, movieId, rating, timestamp)\n",
        "ratings = pd.read_csv('ratings.csv')  # Path to the MovieLens dataset file\n",
        "# (The dataset can be downloaded from the MovieLens website if not already available)\n",
        "\n",
        "# Create contiguous indices for users and movies\n",
        "user_ids = ratings['userId'].unique()\n",
        "movie_ids = ratings['movieId'].unique()\n",
        "num_users = len(user_ids)\n",
        "num_items = len(movie_ids)\n",
        "user_id_to_idx = {uid: idx for idx, uid in enumerate(user_ids)}\n",
        "movie_id_to_idx = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
        "ratings['user_idx'] = ratings['userId'].map(user_id_to_idx)\n",
        "ratings['movie_idx'] = ratings['movieId'].map(movie_id_to_idx)\n",
        "\n",
        "# Split into train and test sets (leave-one-out by latest timestamp per user)\n",
        "ratings = ratings.sort_values(['user_idx', 'timestamp'])\n",
        "test_indices = ratings.groupby('user_idx').tail(1).index  # last interaction for each user\n",
        "train_indices = ratings.index.difference(test_indices)\n",
        "train_df = ratings.loc[train_indices].reset_index(drop=True)\n",
        "test_df = ratings.loc[test_indices].reset_index(drop=True)\n",
        "\n",
        "# Construct edge index for user-item graph using training interactions\n",
        "# We use a homogeneous graph representation:\n",
        "# user nodes 0..num_users-1 and item nodes num_users..num_users+num_items-1.\n",
        "user_index_tensor = torch.tensor(train_df['user_idx'].values, dtype=torch.long)\n",
        "item_index_tensor = torch.tensor(train_df['movie_idx'].values, dtype=torch.long)\n",
        "item_index_tensor += num_users  # offset item indices\n",
        "edge_index = torch.stack([user_index_tensor, item_index_tensor], dim=0)  # shape [2, num_train_edges]\n",
        "\n",
        "# Prepare a dictionary of test interactions for evaluation (per user index)\n",
        "test_interactions = test_df.groupby('user_idx')['movie_idx'].apply(list).to_dict()\n",
        "\n",
        "# (Optionally) also prepare train interactions dict for convenience (e.g. for filtering known items)\n",
        "train_interactions = train_df.groupby('user_idx')['movie_idx'].apply(list).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch_geometric.nn import LGConv\n",
        "\n",
        "class GNNRecommender(nn.Module):\n",
        "    def __init__(self, num_users, num_items, embedding_dim=64, num_layers=3):\n",
        "        super(GNNRecommender, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.total_nodes = num_users + num_items\n",
        "        # Initial trainable embeddings for all users and items:\n",
        "        self.embedding = nn.Embedding(self.total_nodes, embedding_dim)\n",
        "        # Define graph convolution layers (LightGCN propagation layers):\n",
        "        self.convs = nn.ModuleList([LGConv() for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, edge_index):\n",
        "        # Begin with the initial embeddings for all nodes\n",
        "        x = self.embedding.weight  # shape [total_nodes, embedding_dim]\n",
        "        all_layer_embeddings = [x]  # list to collect embeddings at each layer\n",
        "        # GNN propagation through each layer\n",
        "        for conv in self.convs:\n",
        "            # LGConv performs normalized neighbor aggregation\n",
        "            x = conv(x, edge_index)\n",
        "            all_layer_embeddings.append(x)\n",
        "        # Combine embeddings from all layers (including initial).\n",
        "        # LightGCN uses a weighted sum; here we use an equal-weight average for simplicity.\n",
        "        all_embeddings = torch.stack(all_layer_embeddings, dim=0)  # shape [num_layers+1, total_nodes, emb_dim]\n",
        "        final_embeddings = all_embeddings.mean(dim=0)              # shape [total_nodes, emb_dim]\n",
        "        # Split the combined embeddings back into user and item embeddings\n",
        "        user_embeds = final_embeddings[:self.num_users]            # [num_users, emb_dim]\n",
        "        item_embeds = final_embeddings[self.num_users:]            # [num_items, emb_dim]\n",
        "        return user_embeds, item_embeds\n"
      ],
      "metadata": {
        "id": "yijwR1D2Hmkh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_model(model, edge_index, train_interactions, num_users, num_items, epochs=10, lr=0.01, batch_size=1024):\n",
        "    print(\"[INFO] Starting optimized training...\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    edge_index = edge_index.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "\n",
        "    # Precompute user->set of interacted items\n",
        "    user_to_items = {u: set(items) for u, items in train_interactions.items()}\n",
        "\n",
        "    # Precompute negatives once\n",
        "    negatives_per_user = {\n",
        "        u: list(set(range(num_items)) - set(items))\n",
        "        for u, items in train_interactions.items()\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"\\n[INFO] Epoch {epoch}/{epochs}\")\n",
        "        total_loss = 0.0\n",
        "        triplets = []\n",
        "\n",
        "        # === Build training triplets (u, i, j) ===\n",
        "        for u, pos_items in train_interactions.items():\n",
        "            neg_items = negatives_per_user.get(u, [])\n",
        "            if not neg_items:\n",
        "                continue\n",
        "            for pos_item in pos_items:\n",
        "                neg_item = random.choice(neg_items)\n",
        "                triplets.append((u, pos_item, neg_item))\n",
        "\n",
        "        print(f\"[DEBUG] Sampled {len(triplets)} training triplets\")\n",
        "\n",
        "        # === Batch training ===\n",
        "        for i in range(0, len(triplets), batch_size):\n",
        "            batch = triplets[i:i+batch_size]\n",
        "            u_batch = torch.tensor([u for u, _, _ in batch], dtype=torch.long, device=device)\n",
        "            i_batch = torch.tensor([i for _, i, _ in batch], dtype=torch.long, device=device)\n",
        "            j_batch = torch.tensor([j for _, _, j in batch], dtype=torch.long, device=device)\n",
        "\n",
        "            # ‚ùó MOVE FORWARD PASS HERE\n",
        "            user_emb, item_emb = model(edge_index)\n",
        "            user_emb = user_emb.to(device)\n",
        "            item_emb = item_emb.to(device)\n",
        "\n",
        "            u_vec = user_emb[u_batch]\n",
        "            i_vec = item_emb[i_batch]\n",
        "            j_vec = item_emb[j_batch]\n",
        "\n",
        "            pos_score = torch.sum(u_vec * i_vec, dim=1)\n",
        "            neg_score = torch.sum(u_vec * j_vec, dim=1)\n",
        "\n",
        "            loss = -torch.mean(torch.log(torch.sigmoid(pos_score - neg_score)))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / (len(triplets) // batch_size + 1)\n",
        "        print(f\"[INFO] Epoch {epoch} completed | Avg BPR Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "H__TzV9yH0sO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_topN(user_idx, model, edge_index, train_interactions, N=10):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device  # Get model's device (CPU or CUDA)\n",
        "\n",
        "    user_idx_tensor = torch.tensor([user_idx], dtype=torch.long).to(device)\n",
        "    edge_index = edge_index.to(device)\n",
        "\n",
        "    # Compute embeddings on the right device\n",
        "    user_emb, item_emb = model(edge_index)\n",
        "\n",
        "    scores = torch.matmul(user_emb[user_idx_tensor], item_emb.T).squeeze(0)\n",
        "\n",
        "    scores = scores.cpu().detach().numpy()  # convert to numpy for sorting\n",
        "    seen_items = set(train_interactions.get(user_idx, []))\n",
        "    for seen in seen_items:\n",
        "        scores[seen] = -1e9\n",
        "\n",
        "    topN_indices = scores.argsort()[-N:][::-1]\n",
        "    recommended_items = [int(idx) for idx in topN_indices]\n",
        "    return recommended_items\n"
      ],
      "metadata": {
        "id": "3xGbjRUaH6Ik"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, edge_index, test_interactions, train_interactions, K=10):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device  # Detect model's device\n",
        "    edge_index = edge_index.to(device)\n",
        "\n",
        "    # Move model and data to the correct device\n",
        "    user_emb, item_emb = model(edge_index)\n",
        "    user_emb = user_emb.to(device)\n",
        "    item_emb = item_emb.to(device)\n",
        "\n",
        "    recalls = []\n",
        "    ndcgs = []\n",
        "\n",
        "    num_users = model.num_users\n",
        "    for u in range(num_users):\n",
        "        true_items = test_interactions.get(u, [])\n",
        "        if not true_items:\n",
        "            continue\n",
        "\n",
        "        scores = torch.matmul(user_emb[u], item_emb.T)\n",
        "        scores = scores.detach().cpu().numpy()  # convert to numpy for ranking\n",
        "\n",
        "        # Filter out training items\n",
        "        for train_item in train_interactions.get(u, []):\n",
        "            scores[train_item] = -1e9\n",
        "\n",
        "        topk_idx = np.argpartition(scores, -K)[-K:]\n",
        "        topk_idx = topk_idx[np.argsort(scores[topk_idx])][::-1]\n",
        "\n",
        "        # Evaluate Recall@K and NDCG@K\n",
        "        true_set = set(true_items)\n",
        "        hits = 0\n",
        "        dcg = 0.0\n",
        "        idcg = 0.0\n",
        "\n",
        "        for rank, idx in enumerate(topk_idx):\n",
        "            if idx in true_set:\n",
        "                hits += 1\n",
        "                dcg += 1.0 / np.log2(rank + 2)\n",
        "        for rank in range(min(len(true_set), K)):\n",
        "            idcg += 1.0 / np.log2(rank + 2)\n",
        "\n",
        "        recalls.append(hits / len(true_set))\n",
        "        ndcgs.append(dcg / idcg if idcg > 0 else 0.0)\n",
        "\n",
        "    avg_recall = np.mean(recalls)\n",
        "    avg_ndcg = np.mean(ndcgs)\n",
        "    print(f\"Recall@{K}: {avg_recall:.4f}, NDCG@{K}: {avg_ndcg:.4f}\")\n",
        "    return avg_recall, avg_ndcg\n"
      ],
      "metadata": {
        "id": "KhnfW0ljImsO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = GNNRecommender(num_users, num_items, embedding_dim=128, num_layers=3)\n",
        "\n",
        "# Train the model on the training graph\n",
        "train_model(model, edge_index, train_interactions, num_users, num_items, epochs=30, lr=0.005, batch_size=2048)\n",
        "\n",
        "# Generate Top-5 recommendations for user with index 0\n",
        "rec_movies = recommend_topN(user_idx=0, model=model, edge_index=edge_index, train_interactions=train_interactions, N=5)\n",
        "print(f\"Top-5 recommended movies for user 0: {rec_movies}\")\n",
        "\n",
        "# Evaluate the model performance with Recall@10 and NDCG@10\n",
        "evaluate_model(model, edge_index, test_interactions, train_interactions, K=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvG8KrFLIpMs",
        "outputId": "23921073-d73a-49d1-e9af-517c4a16468a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Starting optimized training...\n",
            "\n",
            "[INFO] Epoch 1/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 1 completed | Avg BPR Loss: 0.8072\n",
            "\n",
            "[INFO] Epoch 2/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 2 completed | Avg BPR Loss: 0.7751\n",
            "\n",
            "[INFO] Epoch 3/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 3 completed | Avg BPR Loss: 0.7444\n",
            "\n",
            "[INFO] Epoch 4/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 4 completed | Avg BPR Loss: 0.7168\n",
            "\n",
            "[INFO] Epoch 5/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 5 completed | Avg BPR Loss: 0.6876\n",
            "\n",
            "[INFO] Epoch 6/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 6 completed | Avg BPR Loss: 0.6577\n",
            "\n",
            "[INFO] Epoch 7/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 7 completed | Avg BPR Loss: 0.6249\n",
            "\n",
            "[INFO] Epoch 8/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 8 completed | Avg BPR Loss: 0.5901\n",
            "\n",
            "[INFO] Epoch 9/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 9 completed | Avg BPR Loss: 0.5519\n",
            "\n",
            "[INFO] Epoch 10/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 10 completed | Avg BPR Loss: 0.5069\n",
            "\n",
            "[INFO] Epoch 11/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 11 completed | Avg BPR Loss: 0.4634\n",
            "\n",
            "[INFO] Epoch 12/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 12 completed | Avg BPR Loss: 0.4195\n",
            "\n",
            "[INFO] Epoch 13/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 13 completed | Avg BPR Loss: 0.3751\n",
            "\n",
            "[INFO] Epoch 14/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 14 completed | Avg BPR Loss: 0.3348\n",
            "\n",
            "[INFO] Epoch 15/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 15 completed | Avg BPR Loss: 0.3018\n",
            "\n",
            "[INFO] Epoch 16/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 16 completed | Avg BPR Loss: 0.2712\n",
            "\n",
            "[INFO] Epoch 17/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 17 completed | Avg BPR Loss: 0.2457\n",
            "\n",
            "[INFO] Epoch 18/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 18 completed | Avg BPR Loss: 0.2245\n",
            "\n",
            "[INFO] Epoch 19/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 19 completed | Avg BPR Loss: 0.2077\n",
            "\n",
            "[INFO] Epoch 20/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 20 completed | Avg BPR Loss: 0.1929\n",
            "\n",
            "[INFO] Epoch 21/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 21 completed | Avg BPR Loss: 0.1791\n",
            "\n",
            "[INFO] Epoch 22/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 22 completed | Avg BPR Loss: 0.1685\n",
            "\n",
            "[INFO] Epoch 23/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 23 completed | Avg BPR Loss: 0.1558\n",
            "\n",
            "[INFO] Epoch 24/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 24 completed | Avg BPR Loss: 0.1467\n",
            "\n",
            "[INFO] Epoch 25/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 25 completed | Avg BPR Loss: 0.1395\n",
            "\n",
            "[INFO] Epoch 26/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 26 completed | Avg BPR Loss: 0.1318\n",
            "\n",
            "[INFO] Epoch 27/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 27 completed | Avg BPR Loss: 0.1251\n",
            "\n",
            "[INFO] Epoch 28/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 28 completed | Avg BPR Loss: 0.1190\n",
            "\n",
            "[INFO] Epoch 29/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 29 completed | Avg BPR Loss: 0.1128\n",
            "\n",
            "[INFO] Epoch 30/30\n",
            "[DEBUG] Sampled 100226 training triplets\n",
            "[INFO] Epoch 30 completed | Avg BPR Loss: 0.1086\n",
            "Top-5 recommended movies for user 0: [753, 322, 478, 744, 234]\n",
            "Recall@10: 0.0262, NDCG@10: 0.0111\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.02622950819672131), np.float64(0.011120877969428398))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model state_dict\n",
        "torch.save(model.state_dict(), \"gnn_model.pt\")\n",
        "print(\"[INFO] Model saved to gnn_model.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u44Dol7yJMmV",
        "outputId": "863808bd-3449-44fa-91b2-e439b76f71dd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model saved to gnn_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KHIekyHLRvhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}